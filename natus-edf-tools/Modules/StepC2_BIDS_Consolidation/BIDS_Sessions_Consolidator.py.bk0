#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
session_schema_generator.py

Generates a schema for organizing EDF sessions from arbitrary subfolders,
detects duplicate sessions, and optionally applies a standardized BIDS-like
folder/file structure. Also outputs scans TSV/JSON for the subject.
"""

import os
import sys
import argparse
import logging
import hashlib
import shutil
import json
import struct
from pathlib import Path
import pandas as pd
import zipfile
import rarfile
import py7zr

# EDF file extensions (including archives)
EDF_EXTS = ['.edf', '.edf.gz', '.edf.zip', '.edf.rar', '.edf.7z']

# Logging setup
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def compute_md5(path: Path) -> str:
    """Compute or read MD5 checksum for a file."""
    md5_file = path.with_suffix(path.suffix + '.md5')
    if md5_file.exists():
        try:
            return md5_file.read_text().strip().split()[0]
        except Exception:
            pass
    hash_md5 = hashlib.md5()
    with path.open('rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            hash_md5.update(chunk)
    checksum = hash_md5.hexdigest()
    try:
        md5_file.write_text(f"{checksum}  {path.name}\n")
    except Exception as e:
        logger.warning(f"Could not write MD5 file {md5_file}: {e}")
    return checksum


def get_uncompressed_size(path: Path) -> int:
    """Return uncompressed size for compressed files or file size for others."""
    ext = ''.join(path.suffixes).lower()
    try:
        if ext.endswith('.gz'):
            with open(path, 'rb') as f:
                f.seek(-4, os.SEEK_END)
                return struct.unpack('<I', f.read(4))[0]
        if ext.endswith('.zip'):
            with zipfile.ZipFile(path, 'r') as zf:
                return sum(info.file_size for info in zf.infolist())
        if ext.endswith('.rar'):
            with rarfile.RarFile(path) as rf:
                return sum(info.file_size for info in rf.infolist())
        if ext.endswith('.7z'):
            with py7zr.SevenZipFile(path, 'r') as z:
                return sum(entry.uncompressed for entry in z.list())
    except Exception as e:
        logger.warning(f"Error getting size for {path}: {e}")
    return path.stat().st_size


def parse_input_dirs(input_dirs: list, subject: str) -> pd.DataFrame:
    """Scan each input root, parse all immediate subfolders as sessions."""
    records = []
    for root in input_dirs:
        root = Path(root)
        if not root.is_dir():
            logger.error(f"Input root {root} is not a directory")
            continue
        for sess in sorted([d for d in root.iterdir() if d.is_dir()]):
            # session folder
            sess_files = list(sess.iterdir())
            edf_files = [f for f in sess_files if ''.join(f.suffixes).lower() in EDF_EXTS]
            if not edf_files:
                logger.warning(f"No EDF found in {sess}")
                edf_path = None
            elif len(edf_files) > 1:
                logger.warning(f"Multiple EDFs in {sess}, using {edf_files[0].name}")
                edf_path = edf_files[0]
            else:
                edf_path = edf_files[0]
            # metadata presence
            tsv_present = any(f.suffix.lower() == '.tsv' for f in sess_files)
            json_present = any(f.suffix.lower() == '.json' for f in sess_files)
            # record other files
            other = [f.name for f in sess_files if edf_path and f != edf_path and f.suffix.lower() not in ['.tsv', '.json']]
            # checksum and size
            if edf_path and edf_path.exists():
                size = get_uncompressed_size(edf_path)
                checksum = compute_md5(edf_path)
            else:
                size = None
                checksum = None
            records.append({
                'subject': subject,
                'source_root': str(root),
                'session_folder': sess.name,
                'session_path': str(sess),
                'edf_path': str(edf_path) if edf_path else '',
                'tsv_present': tsv_present,
                'json_present': json_present,
                'other_files': ';'.join(other),
                'file_size': size,
                'checksum': checksum,
                'task': 'unknown'
            })
    return pd.DataFrame(records)


def detect_duplicate_sessions(df: pd.DataFrame) -> pd.DataFrame:
    """Group sessions by file size + checksum to detect duplicates."""
    df = df.copy()
    df['group_id'] = df.groupby(['file_size', 'checksum'], sort=False).ngroup() + 1
    df['action'] = 'keep'
    for gid, grp in df.groupby('group_id'):
        idxs = grp.index.tolist()
        for dup_idx in idxs[1:]:
            df.at[dup_idx, 'action'] = 'skip'
    return df


def assign_global_sessions(df: pd.DataFrame) -> pd.DataFrame:
    """Assign BIDS-style global session numbers based on group_id."""
    df = df.copy()
    df['global_ses'] = df['group_id'].apply(lambda x: f"{x:03d}")
    return df


def generate_schema(df: pd.DataFrame, subject: str, schema_out: str):
    """Write schema Excel and scans TSV/JSON for the subject."""
    df = df.copy()
    df['target_filename'] = df.apply(
        lambda r: f"sub-{subject}_ses-{r['global_ses']}_task-{r['task']}_run-01{Path(r['edf_path']).suffix}",
        axis=1
    )
    schema_df = df[[
        'source_root', 'session_folder', 'edf_path', 'target_filename',
        'action', 'group_id', 'tsv_present', 'json_present', 'other_files',
        'file_size', 'checksum'
    ]]
    # write Excel
    with pd.ExcelWriter(schema_out, engine='openpyxl') as w:
        schema_df.to_excel(w, index=False, sheet_name='Schema')
    logger.info(f"Schema written to {schema_out}")
    # output scans.tsv and scans.json in same folder as schema_out
    out_dir = Path(schema_out).parent
    scans_tsv = out_dir / f"sub-{subject}_scans.tsv"
    scans_json = out_dir / f"sub-{subject}_scans.json"
    scans = schema_df[schema_df['action'] == 'keep'][[
        'target_filename', 'file_size', 'checksum'
    ]]
    scans.to_csv(scans_tsv, sep='\t', index=False)
    logger.info(f"Scans TSV written to {scans_tsv}")
    scans.to_json(scans_json, orient='records', indent=2)
    logger.info(f"Scans JSON written to {scans_json}")


def apply_schema(schema_excel: str, proceed: bool):
    """Apply schema: move and rename session folders/files to BIDS structure."""
    df = pd.read_excel(schema_excel, sheet_name='Schema')
    root = Path(schema_excel).parent
    for _, r in df.iterrows():
        if r['action'] != 'keep':
            continue
        src_folder = Path(r['source_root']) / r['session_folder']
        dest_ieeg = root / f"sub-{r['subject']}" / f"ses-{r['global_ses']}" / 'ieeg'
        dest_ieeg.mkdir(parents=True, exist_ok=True)
        for f in src_folder.iterdir():
            if not f.is_file():
                continue
            suffixes = ''.join(f.suffixes).lower()
            if suffixes in EDF_EXTS:
                dst = dest_ieeg / r['target_filename']
            elif f.suffix == '.tsv':
                dst = dest_ieeg / (r['target_filename'].rsplit('.', 1)[0] + '.tsv')
            elif f.suffix == '.json':
                dst = dest_ieeg / (r['target_filename'].rsplit('.', 1)[0] + '.json')
            else:
                logger.warning(f"Unexpected file {f.name} in {src_folder}")
                dst = dest_ieeg / f.name
            logger.info(f"Moving {f} -> {dst}")
            if proceed:
                shutil.move(str(f), str(dst))
        if proceed:
            try:
                src_folder.rmdir()
            except OSError:
                pass

def main():
    parser = argparse.ArgumentParser(
        description='Generate and apply BIDS-like session schema from arbitrary subfolders.'
    )
    parser.add_argument('--input', nargs='+', required=True,
                        help='Input root directories containing session subfolders.')
    parser.add_argument('--subject', required=True,
                        help='Subject identifier (e.g. 001)')
    parser.add_argument('--schema-output', required=True,
                        help='Path to write schema Excel file.')
    parser.add_argument('--proceed-with-moving', action='store_true',
                        help='If set, move files according to schema.')
    args = parser.parse_args()

    if args.proceed_with_moving:
        apply_schema(args.schema_output, True)
    else:
        df = parse_input_dirs(args.input, args.subject)
        df = detect_duplicate_sessions(df)
        df = assign_global_sessions(df)
        generate_schema(df, args.subject, args.schema_output)

if __name__ == '__main__':
    main()
